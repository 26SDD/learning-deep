{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tlee010/Desktop/github_repos/fastai/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from torchtext import vocab, data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='aclImdb/'\n",
    "names = ['neg','pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,trn_y = texts_from_folders(f'{PATH}train',names)\n",
    "val,val_y = texts_from_folders(f'{PATH}test',names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=trn_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recapping the Logistic Regression (C = constant)\n",
    "\n",
    "Make sure you have a lot of parameters. Linear models can work sometimes, if the underlying assumption of linearity isn't too bad. Works much better with neural networks and more hidden layers. \n",
    "\n",
    "For sentiment, if you look at a set of words, only need a few words to tell us if the documents is either happy or sad. Have lots of parameters and use regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85511999999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='https://www.researchgate.net/profile/Simon_Lin4/publication/271910222/figure/fig1/AS:295071070081024@1447361833118/Fig-1-An-example-of-unigrams-bigrams-trigrams-and-4-grams-extracted-from-the-clinical.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=trn_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: the `x` input is all 0, or counts\n",
    "\n",
    "```python\n",
    "\n",
    "x = [[3 0 0 1...  \n",
    "      0 0 0 1\n",
    "      0 2 1 0   ]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89903999999999995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Naive Bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=trn_y\n",
    "x=trn_term_doc.sign()\n",
    "val_x = val_term_doc.sign()\n",
    "\n",
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How happy or sad everything is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.95208,  0.85605,  0.78485, ...,  3.01678,  0.5028 ,  0.5028 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚Äù .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! ! \"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the ones will be replaced with the log count ratio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a different input\n",
    "\n",
    "```python\n",
    "\n",
    "x = [[0 0    0    0.45 ...  \n",
    "      0 0    0    0.45\n",
    "      0 0.65 0.45 0   ]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768000000000005"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "m = LogisticRegression(dual=True, C=0.1)\n",
    "m.fit(x_nb, y);\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the binarized version better \n",
    "\n",
    "the r measure is a good estimate of how positive or negative it is. Regularization moves coefficients toward 0. 0 is your prior is your expectation about what hte world is like:\n",
    "\n",
    "```\n",
    "Feature Vector\n",
    "[1 1 0 0\n",
    " 1 0 0 0\n",
    " 1 0 0 1 ]\n",
    "\n",
    "r = [0 .4 0 .4]\n",
    "```\n",
    "\n",
    "```\n",
    "Feature Vector x r (positive / negative vectors)\n",
    "[0 .4 0   0\n",
    " 0  0 0   0\n",
    " 0  0 0 -.4]\n",
    "```\n",
    "\n",
    "#### Whats the difference between training on (Feature Vector) vs. (Feature Vector x r)\n",
    "\n",
    "Since the multiplied version. If the cofficients are the same use the R value, which is naive bayes. Otherwise there's a subtle starting point difference. And under regularization, there's a varying effect, and will apply variables differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is called NBSVM\n",
    "\n",
    "Paper: https://nlp.stanford.edu/~sidaw/home/_media/papers:compareacl.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stronger Still"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maximum number of unique words in a review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `wds` - weight decay\n",
    "- `learning rate : 0.02` - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59393a12ee9411f823aa91004672f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.02598  0.11969  0.91618]                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner = md.dotprod_nb_learner()\n",
    "learner.fit(0.02,1,wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eef77a4ac354fbf8d8abfb6479c1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.02027  0.1131   0.92104]                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.02,1,wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Rd iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312651433f0d464cb3aa2c5c4021d6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.01779  0.11128  0.92225]                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(0.02,1,wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Peak under the hood\n",
    "\n",
    "```python\n",
    "class DotProdNB(nn.Module):\n",
    "    def __init__(self, nf, ny, w_adj=0.4, r_adj=10):\n",
    "        super().__init__()\n",
    "        self.w_adj,self.r_adj = w_adj,r_adj\n",
    "        self.w = nn.Embedding(nf+1, 1, padding_idx=0)\n",
    "        self.w.weight.data.uniform_(-0.1,0.1)\n",
    "        self.r = nn.Embedding(nf+1, ny)\n",
    "\n",
    "    def forward(self, feat_idx, feat_cnt, sz):\n",
    "        w = self.w(feat_idx)\n",
    "        r = self.r(feat_idx)\n",
    "        x = ((w+self.w_adj)*r/self.r_adj).sum(1)\n",
    "        return F.softmax(x)\n",
    "\n",
    "```\n",
    "\n",
    "#### This looks very similar to the work we have done before\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The weight adjustment, if not provided, will be `w_adj=0.4`, and for scaling, we will use `r_ad=10`. These constants have been identifyed through experimentation\n",
    "\n",
    "```{r}\n",
    "def __init__(self, nf, ny, w_adj=0.4, r_adj=10):\n",
    "\n",
    "```\n",
    "\n",
    "Matrix multiplication\n",
    "```{r}\n",
    " x = ((w+self.w_adj)*r/self.r_adj).sum(1) <-- this is just a matrix multiplication\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Let's talk about whats happening underneath\n",
    "\n",
    "We want to construct a logistic regression. And currently what we have is a bag of words. But memory becomes an issue now. Even a small corpus can have a large number of documents and a large number of features (unique words).\n",
    "\n",
    "#### Bad way\n",
    "\n",
    "Every unique word is 1-hot encoded:\n",
    "\n",
    "```python\n",
    "\n",
    "terms:\n",
    "he is a dog\n",
    "[1  0 0  0\n",
    " 0  1 0  0\n",
    " 0  0 1  0 \n",
    " 0  0 0  1 ]\n",
    "\n",
    "Doc representation\n",
    "\n",
    "    he is a dog ..... more words\n",
    "docs [                              ]\n",
    "docs [      big matrix x big matrix ]\n",
    "docs [                              ]\n",
    "docs [                              ]\n",
    "```\n",
    "\n",
    "\n",
    "If we multiply the 1-hot encoded by the weight matrix. Selecting that column. A 1-hot matrix multiplier. Looking up a column in an array. \n",
    "\n",
    "\n",
    "#### More efficient way:\n",
    "\n",
    "Don't store the 1-hot encoded variables. Instead we store the docs in a single vector with the term_index (see below)\n",
    "\n",
    "\n",
    "```python\n",
    "doc1: a dog is he : [3,4,2,1]\n",
    "doc2: he is dog : [1,2,4]\n",
    "doc3: dog is dog : [4,2,4]\n",
    "```\n",
    "### \n",
    "1 row for every feature, how every many activations, lets look at each of those word indexes and grab the column out of the weights. **This is in contrast to doing the long matrix multiplication**\n",
    "\n",
    "And these docs will pull the corresponding vector. Each weight vector is the \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Auto Encoder\n",
    "\n",
    "Structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covering Rossman from Deep Learning 3\n",
    "\n",
    "<img src='http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2016/01/rossmann_featured3.png' />\n",
    "\n",
    "Turning time series into an embedding? How big / how wide?\n",
    "\n",
    "<img src='https://www.cse.wustl.edu/~ychen/cenn/cenn.png' style='width:600px'/>\n",
    "\n",
    "#### Cardinality of the features:\n",
    "\n",
    "```python\n",
    "[('Store', 1116),\n",
    " ('DayOfWeek', 8),\n",
    " ('Year', 4),\n",
    " ('Month', 13),\n",
    " ('Day', 32),\n",
    " ('StateHoliday', 3),\n",
    " ('CompetitionMonthsOpen', 26),\n",
    " ('Promo2Weeks', 27),\n",
    " ('StoreType', 5),\n",
    " ('Assortment', 4),\n",
    " ('PromoInterval', 4),\n",
    " ('CompetitionOpenSinceYear', 24),\n",
    " ('Promo2SinceYear', 9),\n",
    " ('State', 13),\n",
    " ('Week', 53),\n",
    " ('Events', 22),\n",
    " ('Promo_fw', 7),\n",
    " ('Promo_bw', 7),\n",
    " ('StateHoliday_fw', 4),\n",
    " ('StateHoliday_bw', 4),\n",
    " ('SchoolHoliday_fw', 9),\n",
    " ('SchoolHoliday_bw', 9)]\n",
    "```\n",
    "\n",
    "#### Embedding sizes (per each field)\n",
    "\n",
    "```python\n",
    "[(1116, 50),\n",
    " (8, 4),\n",
    " (4, 2),\n",
    " (13, 7),\n",
    " (32, 16),\n",
    " (3, 2),\n",
    " (26, 13),\n",
    " (27, 14),\n",
    " (5, 3),\n",
    " (4, 2),\n",
    " (4, 2),\n",
    " (24, 12),\n",
    " (9, 5),\n",
    " (13, 7),\n",
    " (53, 27),\n",
    " (22, 11),\n",
    " (7, 4),\n",
    " (7, 4),\n",
    " (4, 2),\n",
    " (4, 2),\n",
    " (9, 5),\n",
    " (9, 5)]\n",
    "```\n",
    "\n",
    "But First lets look at the feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering - Rossman\n",
    "\n",
    "#### Bring everything you can think of!\n",
    "\n",
    "```\n",
    "train, store, store_states, state_names, googletrend, weather, test = tables\n",
    "```\n",
    "#### Turn the features into human readable\n",
    "\n",
    "```\n",
    "train.StateHoliday = train.StateHoliday!='0'\n",
    "test.StateHoliday = test.StateHoliday!='0'\n",
    "```\n",
    "\n",
    "#### Note about adding in additional data, always check to see if there's things that don't match:\n",
    "\n",
    "A small wrapper for the `pandas.DataFrame.merge` or LEFT JOIN\n",
    "```\n",
    "def join_df(left, right, left_on, right_on=None, suffix='_y'):\n",
    "    if right_on is None: right_on = left_on\n",
    "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
    "                      suffixes=(\"\", suffix))\n",
    "```\n",
    "\n",
    "- Could have the wrong number of rows\n",
    "- or missing indexes\n",
    "\n",
    "```\n",
    "store = join_df(store, store_states, \"Store\")\n",
    "len(store[store.State.isnull()])\n",
    "```\n",
    "Also note, check that don't lose rows from doing repeated left joins. Also avoid cartesian joins (non-unique join fields) aka MAKE new rows. Also from the merge, be sure to remove the duplicate columns\n",
    "\n",
    "#### Memory:\n",
    "\n",
    "If your joins take too long, consider loading into a SQL database for better joins. Pandas is all in ram, and SQL is designed not to work all in memory, so can handle larger datasets. \n",
    "\n",
    "Though when working from strings, you may consider doing hte processing in pandas, as you have full access to the python string library tools\n",
    "\n",
    "#### Data that was added by joins:\n",
    "\n",
    "1. state names\n",
    "2. Google trend on weather\n",
    "3. parse out the different date elements, such as day of the week, month etc.\n",
    "4. Google trend for state\n",
    "5. google trend for all of germany\n",
    "6. days since last competition open since (calculation)\n",
    "\n",
    "\n",
    "### Categorical vs. Continuous\n",
    "\n",
    "If we treat something as a category, we will be giving it an embedding matrix. We will essentially be developing a description vector. If its continuous, we need to find a underlying function that relates to it. Words easily fall into this categorical.\n",
    "\n",
    "Some more examples of embeddings. Note that these different color categories are arbitrary and must be interpretted after the fact. The dimension (how many colors, or royalty etc. ) must be chosen beforehand and trained on a deep learning\n",
    "\n",
    "<img src='https://adriancolyer.files.wordpress.com/2016/04/word2vec-distributed-representation.png?w=600' />\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*6QeFxrI6bXni26Qs.png' />\n",
    "\n",
    "\n",
    "### How do you deal with dates?Turning day of the week data into  an embedding\n",
    "\n",
    "Proximity to holiday. Either upcoming or previous.\n",
    "- How long has it been since the last promotion?\n",
    "- How long till the next promotion?\n",
    "\n",
    "```python\n",
    "def get_elapsed(fld, pre):\n",
    "    day1 = np.timedelta64(1, 'D')\n",
    "    last_date = np.datetime64()\n",
    "    last_store = 0\n",
    "    res = []\n",
    "\n",
    "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
    "        if s != last_store:\n",
    "            last_date = np.datetime64()\n",
    "            last_store = s\n",
    "        if v: last_date = d\n",
    "        res.append(((d-last_date).astype('timedelta64[D]') / day1).astype(int))\n",
    "    df[pre+fld] = res\n",
    "```\n",
    "\n",
    "Using the function for state holiday and school holiday\n",
    "\n",
    "```\n",
    "fld = 'SchoolHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "fld = 'StateHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(fld, 'Before')\n",
    "```\n",
    "\n",
    "#### Moving Averages (take a 7 day period ... create a rolling sum) AKA window functions\n",
    "\n",
    "```python\n",
    "bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "# or wherever you have saved the repo\n",
    "sys.path.append('/Users/tlee010/Desktop/github_repos/fastai/')\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the Last Lesson (data conditioning / changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note `%time` will provide processing times. It's a jupyter notebook function.\n",
    "\n",
    "Ideally only run once to save to disk (takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 572 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n",
      "load complete\n",
      "transform complete\n",
      "export complete\n"
     ]
    }
   ],
   "source": [
    "# loading the data (large)\n",
    "%time df_raw = pd.read_csv('/Users/tlee010/kaggle/bulldozers/Train.csv', low_memory=False, parse_dates=[\"saledate\"])\n",
    "print('load complete')\n",
    "\n",
    "# transform the data\n",
    "df_raw.SalePrice = np.log(df_raw.SalePrice)\n",
    "train_cats(df_raw)\n",
    "df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\n",
    "add_datepart(df_raw, 'saledate')\n",
    "print('transform complete')\n",
    "\n",
    "# export\n",
    "df_raw.to_feather('/tmp/raw')\n",
    "print('export complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note, edit the path of the tmp file where necessary\n",
    "df_raw = pd.read_feather('/tmp/raw')\n",
    "df_trn, y_trn = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digging into Random Forests for lec 3 + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some datasets they work really well. But how do they actually work. What can we tune, and improve.\n",
    "\n",
    "Second, how can we interpret the results and learn something about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review up to this point\n",
    "\n",
    "### LastLec: Talking about the fastai library\n",
    "\n",
    "Using state-of-the-art techinques packaged together. The library will wrap existing libraries. Many of sklearn libraries are used under the hood. \n",
    "\n",
    "https://github.com/fastai/fastai\n",
    "\n",
    "#### How to use it\n",
    "\n",
    "1. Put notebooks in the same directory as fastai. \n",
    "2. Symlink it\n",
    "3. copy the git repo to your dev folder\n",
    "4. or append the path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LastLec: Make a Symlink in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ln -s ../../fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LastLec: Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### LastLec: Understanding Metrics\n",
    " \n",
    " For the bulldozers dataset, the kaggle says the target metric is RMSE\n",
    " \n",
    " $$RMSE = sum(log(actuals)-log(preds))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### LastLec: data conditioning\n",
    "    \n",
    "1. **Breakout dates:** Date column, replace with a number of columns using `date_part()` which is another fastai custom function\n",
    "\n",
    "2. **Replace strings with categories**: using `train_cat(dataframe)` that will go through each field and clean. Then follow up and change the levels \n",
    "\n",
    "3. **Then separate target and features** - `df, y = prod_df(dataframe, \"target field\")`\n",
    "\n",
    "4. **Then traing the model**\n",
    "\n",
    "```python\n",
    "# creates a blank model\n",
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# fits the model and sets the coefficients based on \n",
    "# data\n",
    "model.fit(df,y)\n",
    "\n",
    "# scores the model by generating y_hats\n",
    "model.score(df,y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is R^2? (accuracy)\n",
    "\n",
    "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).[1]\n",
    "\n",
    "$$ SS_{res} = \\sum{(f_i-\\bar{y})^2} $$\n",
    "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "**Range of values for R^2 ** - can be less than 1. If you predict infinity for each values, you will get negative. Which means your model is worse than predicting the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Careful about overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/alexeygrigorev/wiki-figures/master/ufrt/kddm/overfitting-logreg-ex.png\" alt=\"\" style=\"width: 70%\"/>\n",
    "<center>\n",
    "[Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation is one of the important part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((389125, 66), (389125,), (12000, 66))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, y = proc_df(df_raw, 'SalePrice')\n",
    "\n",
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 12000  # same as Kaggle's test set size\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats the difference between test and Validation Set\n",
    "\n",
    "** Test set** - in this context, test is a small subset of train, designed to iterate.\n",
    "\n",
    "** Validation ** - final score of the best model from the iterations of model design with test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LastLec: Ran the Random Forest Regressor\n",
    "\n",
    "```python\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "\n",
    "```python\n",
    "CPU times: user 1min 4s, sys: 376 ms, total: 1min 4s\n",
    "Wall time: 8.56 s\n",
    "[0.0902435011024215, 0.2507924131328525, 0.98295721706791372, 0.88767491329235182]\n",
    "```\n",
    "\n",
    "#### Discussion\n",
    "|| Train score | Test Score |\n",
    "|---|------|-------|\n",
    "|RMSE| 0.090 | 0.251  |\n",
    "|R^2|0.983  | 0.887 |\n",
    "\n",
    "We see some good scores, but with the differences, we see that we are overfitting\n",
    "\n",
    "#### Appendix - how the score worked \n",
    "\n",
    "This custom print score gives:\n",
    "\n",
    "- RMSE_train\n",
    "- RMSE_test\n",
    "- Correlation of Train\n",
    "- Correlation of Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LastLec: Speeding up the processing time - Look at a subset.\n",
    "\n",
    "Randomly sample from the overall, and need to ensure that our validation and train set are coming from the same distribution. We will work with a smaller subset so that we can iterate much faster through the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "df_trn, y_trn = proc_df(df_raw, 'SalePrice', subset=30000)\n",
    "X_train, _ = split_vals(df_trn, 20000)\n",
    "y_train, _ = split_vals(y_trn, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Single Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5339102994711508, 0.5665289699293733, 0.40769661682316749, 0.42681842757941629]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastAI: Custom function draw_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??draw_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree diagram below shows:\n",
    "\n",
    "- **`field <= criteria`**: the feature + criteria per fields. Criteria can be either a categorical field, or a continous\n",
    "- **`sample`**: number of samples\n",
    "- **`value`**: the mean value \n",
    "- **`mse`**: the difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"434pt\"\n",
       " viewBox=\"0.00 0.00 720.00 434.45\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.813585 0.813585) rotate(0) translate(4 530)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-530 880.972,-530 880.972,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.690196\" stroke=\"black\" points=\"166.487,-295 25.4565,-295 25.4565,-231 166.487,-231 166.487,-295\"/>\n",
       "<text text-anchor=\"start\" x=\"33.2146\" y=\"-279.8\" font-family=\"Times,serif\" font-size=\"14.00\">Coupler_System ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"61.4983\" y=\"-265.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.481</text>\n",
       "<text text-anchor=\"start\" x=\"48.4724\" y=\"-251.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 20000</text>\n",
       "<text text-anchor=\"start\" x=\"54.1155\" y=\"-237.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.096</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.764706\" stroke=\"black\" points=\"391.337,-346 264.607,-346 264.607,-282 391.337,-282 391.337,-346\"/>\n",
       "<text text-anchor=\"start\" x=\"272.539\" y=\"-330.8\" font-family=\"Times,serif\" font-size=\"14.00\">YearMade ≤ 1991.5</text>\n",
       "<text text-anchor=\"start\" x=\"293.498\" y=\"-316.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.412</text>\n",
       "<text text-anchor=\"start\" x=\"280.472\" y=\"-302.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 17762</text>\n",
       "<text text-anchor=\"start\" x=\"286.116\" y=\"-288.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.209</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.604,-278.44C194.276,-284.576 226.123,-291.638 254.107,-297.843\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"253.706,-301.339 264.227,-300.087 255.222,-294.505 253.706,-301.339\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.141\" y=\"-309.319\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.125490\" stroke=\"black\" points=\"391.337,-244 264.607,-244 264.607,-180 391.337,-180 391.337,-244\"/>\n",
       "<text text-anchor=\"start\" x=\"272.539\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\">YearMade ≤ 1998.5</text>\n",
       "<text text-anchor=\"start\" x=\"293.498\" y=\"-214.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.121</text>\n",
       "<text text-anchor=\"start\" x=\"283.972\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2238</text>\n",
       "<text text-anchor=\"start\" x=\"289.616\" y=\"-186.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.199</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.604,-247.56C194.276,-241.424 226.123,-234.362 254.107,-228.157\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.222,-231.495 264.227,-225.913 253.706,-224.661 255.222,-231.495\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.141\" y=\"-208.281\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.611765\" stroke=\"black\" points=\"634.981,-465 514.963,-465 514.963,-401 634.981,-401 634.981,-465\"/>\n",
       "<text text-anchor=\"start\" x=\"522.718\" y=\"-449.8\" font-family=\"Times,serif\" font-size=\"14.00\">ModelID ≤ 4520.5</text>\n",
       "<text text-anchor=\"start\" x=\"540.498\" y=\"-435.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.372</text>\n",
       "<text text-anchor=\"start\" x=\"530.972\" y=\"-421.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 7712</text>\n",
       "<text text-anchor=\"start\" x=\"536.616\" y=\"-407.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.967</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.43,-344.337C426.178,-361.214 469.489,-382.251 505.202,-399.597\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.058,-402.933 514.582,-404.154 507.117,-396.636 504.058,-402.933\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.878431\" stroke=\"black\" points=\"657.318,-346 492.626,-346 492.626,-282 657.318,-282 657.318,-346\"/>\n",
       "<text text-anchor=\"start\" x=\"500.549\" y=\"-330.8\" font-family=\"Times,serif\" font-size=\"14.00\">fiProductClassDesc ≤ 13.5</text>\n",
       "<text text-anchor=\"start\" x=\"540.498\" y=\"-316.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.364</text>\n",
       "<text text-anchor=\"start\" x=\"527.472\" y=\"-302.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 10050</text>\n",
       "<text text-anchor=\"start\" x=\"533.116\" y=\"-288.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.395</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.43,-314C418.942,-314 451.823,-314 482.051,-314\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"482.188,-317.5 492.188,-314 482.188,-310.5 482.188,-317.5\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.756863\" stroke=\"black\" points=\"857.971,-526 753.973,-526 753.973,-476 857.971,-476 857.971,-526\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-510.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.377</text>\n",
       "<text text-anchor=\"start\" x=\"761.972\" y=\"-496.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3984</text>\n",
       "<text text-anchor=\"start\" x=\"764.116\" y=\"-482.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.201</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M635.243,-450.6C668.654,-460.521 710.315,-472.892 744,-482.895\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"743.144,-486.292 753.726,-485.783 745.136,-479.581 743.144,-486.292\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.454902\" stroke=\"black\" points=\"857.971,-458 753.973,-458 753.973,-408 857.971,-408 857.971,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-442.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.247</text>\n",
       "<text text-anchor=\"start\" x=\"761.972\" y=\"-428.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3728</text>\n",
       "<text text-anchor=\"start\" x=\"767.616\" y=\"-414.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.718</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M635.243,-433C668.51,-433 709.957,-433 743.566,-433\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"743.726,-436.5 753.726,-433 743.726,-429.5 743.726,-436.5\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.690196\" stroke=\"black\" points=\"857.971,-390 753.973,-390 753.973,-340 857.971,-340 857.971,-390\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-374.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.202</text>\n",
       "<text text-anchor=\"start\" x=\"761.972\" y=\"-360.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3904</text>\n",
       "<text text-anchor=\"start\" x=\"764.116\" y=\"-346.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.096</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.354,-332.124C685.852,-338.471 717.345,-345.485 743.898,-351.398\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"743.294,-354.85 753.816,-353.607 744.816,-348.017 743.294,-354.85\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"857.971,-322 753.973,-322 753.973,-272 857.971,-272 857.971,-322\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-306.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.374</text>\n",
       "<text text-anchor=\"start\" x=\"761.972\" y=\"-292.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 6146</text>\n",
       "<text text-anchor=\"start\" x=\"764.116\" y=\"-278.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 10.585</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.354,-307.959C685.727,-305.852 717.068,-303.526 743.547,-301.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"744.103,-305.028 753.816,-300.798 743.584,-298.048 744.103,-305.028\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.007843\" stroke=\"black\" points=\"653.518,-244 496.426,-244 496.426,-180 653.518,-180 653.518,-244\"/>\n",
       "<text text-anchor=\"start\" x=\"504.449\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\">fiSecondaryDesc ≤ 154.0</text>\n",
       "<text text-anchor=\"start\" x=\"540.498\" y=\"-214.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.094</text>\n",
       "<text text-anchor=\"start\" x=\"534.472\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 951</text>\n",
       "<text text-anchor=\"start\" x=\"536.872\" y=\"-186.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.011</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.43,-212C420.213,-212 454.873,-212 486.222,-212\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.272,-215.5 496.272,-212 486.272,-208.5 486.272,-215.5\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.211765\" stroke=\"black\" points=\"657.318,-125 492.626,-125 492.626,-61 657.318,-61 657.318,-125\"/>\n",
       "<text text-anchor=\"start\" x=\"500.549\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\">fiProductClassDesc ≤ 40.5</text>\n",
       "<text text-anchor=\"start\" x=\"540.498\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.095</text>\n",
       "<text text-anchor=\"start\" x=\"530.972\" y=\"-81.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1287</text>\n",
       "<text text-anchor=\"start\" x=\"536.616\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.337</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.43,-181.663C424.095,-165.797 464.329,-146.255 498.703,-129.559\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"500.303,-132.673 507.769,-125.156 497.244,-126.377 500.303,-132.673\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"854.471,-254 757.473,-254 757.473,-204 854.471,-204 854.471,-254\"/>\n",
       "<text text-anchor=\"start\" x=\"774.998\" y=\"-238.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.09</text>\n",
       "<text text-anchor=\"start\" x=\"765.472\" y=\"-224.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 918</text>\n",
       "<text text-anchor=\"start\" x=\"767.616\" y=\"-210.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 8.998</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M653.501,-217.755C684.125,-220.029 718.702,-222.596 747.08,-224.702\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.004,-228.206 757.236,-225.456 747.523,-221.225 747.004,-228.206\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.247059\" stroke=\"black\" points=\"852.185,-186 759.759,-186 759.759,-136 852.185,-136 852.185,-186\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-170.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.069</text>\n",
       "<text text-anchor=\"start\" x=\"768.972\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 33</text>\n",
       "<text text-anchor=\"start\" x=\"767.616\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.388</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M653.501,-194.734C685.072,-187.703 720.844,-179.736 749.694,-173.311\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"750.592,-176.697 759.592,-171.106 749.07,-169.864 750.592,-176.697\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.152941\" stroke=\"black\" points=\"854.471,-118 757.473,-118 757.473,-68 854.471,-68 854.471,-118\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.087</text>\n",
       "<text text-anchor=\"start\" x=\"765.472\" y=\"-88.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 647</text>\n",
       "<text text-anchor=\"start\" x=\"767.616\" y=\"-74.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.243</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.354,-93C686.992,-93 719.87,-93 747.06,-93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.184,-96.5001 757.184,-93 747.184,-89.5001 747.184,-96.5001\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.274510\" stroke=\"black\" points=\"854.471,-50 757.473,-50 757.473,-0 854.471,-0 854.471,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"771.498\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.086</text>\n",
       "<text text-anchor=\"start\" x=\"765.472\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 640</text>\n",
       "<text text-anchor=\"start\" x=\"767.616\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 9.431</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.354,-68.8343C687.123,-59.9946 720.159,-50.1845 747.419,-42.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"748.594,-45.3922 757.184,-39.1903 746.601,-38.6818 748.594,-45.3922\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1a28675390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_tree(m.estimators_[0], df_trn, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the tree diagram\n",
    "\n",
    "1. To make a forest, we need to make a tree.\n",
    "2. If we wanted to make a tree from scratch, we need a binary criteria How?\n",
    "\n",
    "**What metric ?** Calculate the RMSE times the number of samples. That would give the weighted average RMSE.\n",
    "\n",
    "**Which variable ? ** Try all variables, try every possible split criteria (within the field) to see which gives the better weighted average RMSE. Then we picked that one.\n",
    "\n",
    "** Split by 3? ** Not necessary, can just do a 2nd binary split again\n",
    "\n",
    "** When does the model stop? ** there's some limit given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make a Forest: Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of little bootstraps\n",
    "\n",
    "Bagging - what if we made 5 models that were little predictions that were not correlated with each other. That would mean the models found different insights. What if we average teh models? we create an 'ensemble score'.\n",
    "\n",
    "1. Create 1 tree\n",
    "2. Take a subset of data\n",
    "3. Build a deep tree (that will overfit)\n",
    "4. repeat 100 times\n",
    "\n",
    "All the trees will be better than nothing, but will all overfit (small samples). But since it is using random samples, will overfit different ways. \n",
    "\n",
    "If we average across all the errors, it will reduce. Then we average all them together. \n",
    "\n",
    "### Sklearn Library reference for RandomForest\n",
    "\n",
    "```python\n",
    "m = RandomForestRegressor(n_estimators=10, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "```\n",
    "\n",
    "**n_estimators** = how many trees\n",
    "\n",
    "**max_depth** = how deep the tree will be\n",
    "\n",
    "**bootstrap** = replacements in the random subsets\n",
    "\n",
    "**n_jobs** = how CPU threads you will use (-1 is all available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://manish-m.com/wp-content/uploads/2012/11/BaggingCropped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11761445042573493, 0.27999064711964805, 0.97125720595526177, 0.8599977476692966]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each of the Trees are housed in `.estimators_`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab the predictions for each individual tree, and look at one example. The example down below goes through each of the estimators and pulls out the predictions. Returns the ARRAY, MEAN, ACTUAL\n",
    "\n",
    "- Note that the array has very bad predictions, but the collective mean is very close to the actual (last two values)\n",
    "\n",
    "\n",
    "\n",
    "`np.stack` - stacks on a new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.04782,  8.9872 ,  9.02401,  9.4727 ,  9.54681,  9.25913,  9.43348,  9.39266,  9.39266,  9.21034,\n",
       "         9.07108,  9.21034,  9.07681,  9.39266,  9.5819 ,  9.43348,  9.30565,  9.4727 ,  9.04782,  8.9872 ]),\n",
       " 9.2673237939361446,\n",
       " 9.1049798563183568)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "\n",
    "# sample of the first row of this new data collection\n",
    "preds[:,0], np.mean(preds[:,0]), y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 trees, 12000 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recent Tree Advances - focus on uncorrelated trees instead of more accurate trees\n",
    "\n",
    "#### Sklearn\n",
    "```python\n",
    "from sklearn.tree import ExtraTreeClassifier, ExtraTreeRegressor\n",
    "```\n",
    "\n",
    "Extremely randomized trees. Much faster, more randomness, build more trees and get better generalization. Similar to random forest, but different focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of R^2 vs. number of trees\n",
    "\n",
    "Based on the plot, it appears that the benefit of adding more trees begins to diminish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4VHE865AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a267b6a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we have 20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10820721018941977, 0.2700912275188356, 0.97567123765159214, 0.86972264537727961]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we keep doubling\n",
    "\n",
    "|trees | R^2|\n",
    "|-----|-----|\n",
    "|1|0.409|\n",
    "|20|0.868|\n",
    "|40|0.870|\n",
    "|80|0.873|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Out of Bag Score - OOB \n",
    "\n",
    "\n",
    "<img src='http://file.scirp.org/Html/6-9101686/036879b1-0ef0-44ea-a061-0e3b48f38e08.jpg' style=\"width:400px\"/>\n",
    "\n",
    "<img src='https://neupsykey.com/wp-content/uploads/2016/11/A324794_1_En_1_Fig17_HTML.gif' />\n",
    "\n",
    "Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n",
    "\n",
    "The idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n",
    "\n",
    "This also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n",
    "\n",
    "This is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below.\n",
    "\n",
    "\n",
    "#### Average all the trees you didn't use for \n",
    "\n",
    "What if you have small datasets. We can run unused rows in different trees. As long as you have enough trees, a sample row will \n",
    "\n",
    "#### note the additional option `oob_score` when defining the RandomForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10225070336002949, 0.26644847729681304, 0.97827597849728065, 0.87321307779312418, 0.84565378571366501]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection.GridSearchCV\n",
    "```\n",
    "\n",
    "A nice implementation to run a lot of different models with different hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trn, y_trn = proc_df(df_raw, 'SalePrice')\n",
    "\n",
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let random forest pull multiple times (randomly) from the superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_rf_samples(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 10 Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.45 s, sys: 598 ms, total: 9.05 s\n",
      "Wall time: 4.24 s\n",
      "[0.23956964398307784, 0.2786502613046347, 0.88005051130538814, 0.86133499128265611, 0.86753169378295814]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 40 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22695440917054677, 0.2629258201187014, 0.89235048562864372, 0.87654336129080646, 0.88085954917203169]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FASTAI: Custom rf_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??set_rf_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def set_rf_samples(n):\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "```\n",
    "\n",
    "To reset the parameters\n",
    "\n",
    "```python\n",
    "reset_rf_samples()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidance\n",
    "\n",
    "Do your machine learning on a reasonable subsample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a Baseline with 40 with nodes to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07839009127394027, 0.2389417161150468, 0.98715727549739707, 0.89803950597792648, 0.90817254019168592]\n"
     ]
    }
   ],
   "source": [
    "reset_rf_samples()\n",
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter 1 : require a number of samples per leaf (e.g. 3 rows)\n",
    "\n",
    "Should cut the number of levels being made. 1,3,5,10,25\n",
    "\n",
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n",
    "\n",
    "- There are less decision rules for each leaf node; simpler models should generalize better\n",
    "- The predictions are made by averaging more rows in the leaf node, resulting in less volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11502352966490124, 0.23317922874081387, 0.9723491677838787, 0.9028981069020966, 0.9084794157320788]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter 2 : max features\n",
    "\n",
    "Max features, less correlated the better. If every tree always has the same field every time, there won't be much variation to the trees. At every split point. Take a different subset of columns.\n",
    "\n",
    "#### 0.5 = randomly choose half (default is to choose all), can pass ^2 or log2 or Sqrt, this guarantees variation\n",
    "\n",
    "We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11919783670771852, 0.22816094374676066, 0.97030580383483711, 0.90703262187844114, 0.91169902407915004]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary of Tree Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.analyticsvidhya.com/wp-content/uploads/2016/02/tree-infographic.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compare our results directly with the Kaggle competition, since it used a different validation set (and we can no longer to submit to this competition) - but we can at least see that we're getting similar results to the winners based on the dataset we have.\n",
    "\n",
    "The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n",
    "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does Batch Norm Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes weights later (say 10th layer) in the NN more robust to changes. Example, you trained a NN on black cats but then later had colored cats later on. The learning algorithm would be very overfit.\n",
    "\n",
    "Covariate shift - if the distribution of X changes, then you may need to change again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Batch norm reduces the shifting of the underlying layers. No matter how much they change, the mean and the variance will remain the same. Even if the distribution changes. It limits the effect of earlier layer impact, also known as more \"stable\". Another way of describe it is : weakening the coupling between layers.\n",
    "\n",
    "2. **Slight regularization side effect**\n",
    "    - Each mini batch is scaled by the mean/variance computed on just that mini-batch\n",
    "    - This adds some noise to the values z within that minibatch. So similar to dropout, it adds some noise to each hidden layer's activations.\n",
    "    - This has a slight regularization effect\n",
    "    - Bigger mini-batch size will reduce the regularization\n",
    "\n",
    "### Using batch norm at test time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\frac{1}{m} \\sum_{i}{Z^{(i)}}$$ \n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i}{(Z - \\mu)} $$\n",
    "\n",
    "$$Z_{norm}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $$\n",
    "\n",
    "$$\\tilde{Z}^{(i)} = \\gamma Z_{norm}^{(i)} + \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come up with some sort of different estimation for mean and variances. $\\mu$ and $\\sigma^2$. Will use a weighted average (across mini-batches). Also known as a running average\n",
    "\n",
    "$$ X^{\\{1\\}},X^{\\{2\\}},X^{\\{3\\}}...$$\n",
    "\n",
    "$$ \\mu^{\\{1\\}},\\mu^{\\{2\\}},\\mu^{\\{3\\}}...$$\n",
    "\n",
    "$$ \\theta^{\\{1\\}},\\theta^{\\{2\\}},\\theta^{\\{3\\}}...$$\n",
    "\n",
    "$$ \\sigma^{\\{1\\}},\\sigma^{\\{2\\}},\\sigma^{\\{3\\}}...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

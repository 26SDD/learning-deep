{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrying out Error Analysis\n",
    "\n",
    "90% accuracy\n",
    "10% error\n",
    "Should you try to make yoru cat classifier to do better on dogs?\n",
    "\n",
    "Error Analysis:\n",
    "Get ~ 100 mislabeled dev set examples. \n",
    "Count up how many are dogs.\n",
    "\n",
    "Say 5% is wrong of dogs, then the 10% model error could only go down to 9.5% but there's only so much that you can improve the model. This is called the ceiling\n",
    "\n",
    "Say 50% is wrong, if you solve the the problem the most the error could drop is 10% down to 5%.\n",
    "\n",
    "\n",
    "#### Parallel Ideas: what if you are trying to improve different aspects all in parallel\n",
    "\n",
    "Ideas for cat detection: \n",
    "\n",
    "- Fix pictures of dogs being recognized as cast\n",
    "- fix great cats (lions, panthers, etc. ) being misrecognized\n",
    "- Improve Performance on blurry images\n",
    "- For each category count the % of hte error is accounted for\n",
    "\n",
    "This can give you an idea of how much improvement you can expect. Give you a sense of how much better you can do. This will also help prioritize.\n",
    "\n",
    "#### What do you do if some of your training data is mislabeled?\n",
    "\n",
    "Is it worthwhile to go through and fix some of these labels? Mislabeled examples? First you should consider a training set, they are robust to random errors. This is given that hte dataset is large enough and the error percentage is low enough. They are less robust to systematic errors. If the mislabelling is consistent, the model will learn the wrong class.\n",
    "\n",
    "Add this as a column in the error analysis. This will also give a % effect of the overall error. Is it worth fixing these errors?\n",
    "\n",
    "3 Numbers to consider: Overall dev set error. % errors due to mislabeled. % of errors due to other causes. So if you \n",
    "Example:\n",
    "\n",
    "- dev set error 10%, and if \n",
    "- 6% are due to mislabeled, then 0.6% overall, \n",
    "- and others are due to 9.4%\n",
    "\n",
    "Second scenario:\n",
    "\n",
    "- Dev Set Error: 2% \n",
    "- Errors due to incorrect labels: 0.6% \n",
    "- Errors due to other causes: 1.4%\n",
    "\n",
    "\n",
    "- Apply same process to your dev and test setes to make sure they continue to come from the same distribution.\n",
    "- Consider examining examples your algorithm got right as well as ones it go wrong. This isnt comprehensive, if you have 98% correct, it will take some time to check all the correct items.\n",
    "- Train and test/dev set now may come from the distribution\n",
    "\n",
    "In building practical systems, there;s still a lot of manual effort still involved. I will still go in to manually count what things are going wrong. This is time well spent. \n",
    "\n",
    "## Build your first system quickly then iterate\n",
    "\n",
    "Build first system quickly and iterate\n",
    "\n",
    "Example: Speech recognition\n",
    "\n",
    "- Noisy background\n",
    "    - Cafe noise\n",
    "    - Care noise\n",
    "- Accented speech\n",
    "- Far from the microphone\n",
    "- Young children's speech\n",
    "- Stuttering\n",
    "\n",
    "1. Set up dev/test set and metric\n",
    "2. build initial system quickly\n",
    "3. Use bias/variance analysis & error analysis to prioritize next steps. The errors will help assist the prioritization of where to target effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing on different distribution\n",
    "\n",
    "More and more teams are training on data that comes from different distributions. \n",
    "\n",
    "#### Example: cat example again\n",
    "\n",
    "- Data from webpages 200,000\n",
    "- Data from mobile apps 10,000\n",
    "\n",
    "If you dont have many users yet, you dont have enough pictures downloaded yet. You care about the mobile app, but you dont have enough examples yet. \n",
    "\n",
    "1. Take both sets of pictures and randomly mix them together. But if you randomly mix these together, the test and the dev sets will come from different distributions. ANd you may spend most of your time optimizing for web images instead of mobile ones.\n",
    "\n",
    "2. Instead, the training set will have 5,000 from mobile, and test and dev will have 2500 images. True the distribution will be different, but those are the images we are looking for\n",
    "\n",
    "#### Example: speed recognition example in the Rear view mirror\n",
    "\n",
    "Training:\n",
    "\n",
    "- Purchasing data X, y\n",
    "- Smart speaker control\n",
    "- Voice keyboard\n",
    "500, 000\n",
    "\n",
    "Dev/Test\n",
    "\n",
    "- speech activated rearview mirror -> 20,000\n",
    "\n",
    "Setup \n",
    "\n",
    "- Set train to be 500k, dev would be 10k, 10k\n",
    "- Or train be 510k and 5k and 5k\n",
    "\n",
    "### Bias and Variance with mismatched data distributions\n",
    "#### Should you always use all the data that you have?\n",
    "\n",
    "- Assume errors get 0% error\n",
    "- Training error .. 1% \n",
    "- Dev error ... 10% \n",
    "\n",
    "Since the data comes from a different distribution, how can you tell if there is room for improvement? \n",
    "\n",
    "Training-dev set (new) : same distribution as training set but not used for training. Carve out part of the trainingset. This is almost like another holdout dataset.\n",
    "\n",
    "- Training error 1%\n",
    "- Training-dev error 9%\n",
    "- Dev error 10%\n",
    "\n",
    "So you know you have a variance issue. It's not generalizing well.\n",
    "\n",
    "- Training error 1%\n",
    "- Training-dev error 1.5%\n",
    "- Dev error 10%\n",
    "\n",
    "Data mismatch problem, because your learning algo was trained on a different dataset. \n",
    "\n",
    "- Human Error 0%\n",
    "- Training error 10%\n",
    "- Training-dev error 11%\n",
    "- Dev error 12%\n",
    "\n",
    "You have a bias issue, because you are doing poorly compared to bayes error\n",
    "\n",
    "- Human Error 0%\n",
    "- Training error 10%\n",
    "- Training-dev error 11%\n",
    "- Dev error 20%\n",
    "\n",
    "You have a bias issue, and a data mismatch issue.\n",
    "\n",
    "\n",
    "- Human level error -> 4% \n",
    "        - Bias error gap\n",
    "- Training set error -> 7% (Variance)\n",
    "        - Variance Error\n",
    "- Training-dev set error -> 10%\n",
    "        - data mismatch\n",
    "- Dev Error -> 12%\n",
    "        - degree of overfitting to the dev set\n",
    "- Dev Error -> 12%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

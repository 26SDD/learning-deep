{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.72:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from user_definition import *\n",
    "#Do not add additional libraries.\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(app_name)\n",
    "sc = SparkContext(conf=conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "from user_definition import *\n",
    "#Do not add additional libraries.\n",
    "\n",
    "# conf = SparkConf().setMaster(\"local[*]\").setAppName(app_name)\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([1,2,3,4,5,6,7,8,9], 3)\n",
    "lines.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.fold(0, lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Odd numbers sum and Count - note this is for single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 3.0, 5.0, 7.0, 9.0]\n",
      "(5, 25.0)\n"
     ]
    }
   ],
   "source": [
    "lines = sc.parallelize([1,2,3,4,5,6,7,8,9], 3)\n",
    "odds = lines.filter( lambda x : x % 2 == 1)\n",
    "odds = odds.map(lambda x : float(x))\n",
    "print odds.collect()\n",
    "res = odds.aggregate((0,0),\n",
    "               lambda x,y: (x[0]+1, x[1]+y),\n",
    "               lambda x,y: (x[0]+y[0], x[1] + y[1])\n",
    "              )\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odd_nums = sc.parallelize([1,3,5,7,9],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Sample and CountByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([3,4,1,2])\n",
    "y = sc.parallelize(range(2,6))\n",
    "z = x.union(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 2: 2, 3: 2, 4: 2, 5: 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.countByValue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.takeSample(withReplacement=False,num=3,seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/README.md'\n",
    "rdm = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Keys and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#', 1),\n",
       " (u'Apache', 1),\n",
       " (u'Spark', 1),\n",
       " (u'Spark', 1),\n",
       " (u'is', 1),\n",
       " (u'a', 1),\n",
       " (u'fast', 1),\n",
       " (u'and', 1),\n",
       " (u'general', 1),\n",
       " (u'cluster', 1),\n",
       " (u'computing', 1),\n",
       " (u'system', 1),\n",
       " (u'for', 1),\n",
       " (u'Big', 1),\n",
       " (u'Data.', 1),\n",
       " (u'It', 1),\n",
       " (u'provides', 1),\n",
       " (u'high-level', 1),\n",
       " (u'APIs', 1),\n",
       " (u'in', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = rdm.flatMap(lambda x: x.split())\n",
    "words = words.map(lambda x: (x,1))\n",
    "words.collect()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#', u'Apache', u'Spark', u'Spark', u'is']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.keys().collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.values().collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "Create a pair RDD with (length of a word, list of words) from README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, [u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'])\n",
      "----------------\n",
      "(112, [u'[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)'])\n",
      "----------------\n",
      "(2, [u'is', u'It', u'in', u'R,', u'an', u'It', u'of', u'##', u'on', u'##', u'is', u'To', u'do', u'to', u'do', u'if', u'by', u'-T', u'in', u'is', u'at', u'an', u'##', u'to', u'is', u'to', u'##', u'if', u'##', u'in', u'To', u'of', u'Pi', u'to', u'to', u'be', u'or', u'to', u'on', u'to', u'or', u'to', u'an', u'if', u'is', u'in', u'of', u'if', u'no', u'##', u'is', u'be', u'on', u'to', u'or', u'##', u'to', u'to', u'in', u'of', u'to', u'at', u'on', u'of', u'##', u'to', u'in', u'an', u'on', u'to'])\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "words = rdm.flatMap(lambda x: x.split())\n",
    "words = words.map(lambda x: (len(x),x))\n",
    "res = words.groupByKey().map(lambda x: (x[0],list(x[1])))\n",
    "\n",
    "for x in res.collect()[:3]:\n",
    "    print x[:2]\n",
    "    print '----------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 22),\n",
       " (u'Spark', 15),\n",
       " (u'to', 14),\n",
       " (u'for', 11),\n",
       " (u'and', 11),\n",
       " (u'##', 8),\n",
       " (u'a', 8),\n",
       " (u'run', 7),\n",
       " (u'can', 7),\n",
       " (u'is', 6)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = rdm.flatMap(lambda x: x.split())\n",
    "words = words.map(lambda x: (x,1))\n",
    "res = words.groupByKey()\n",
    "res2 = res.mapValues(sum)\n",
    "res2.sortBy(lambda x: x[1], ascending = False).collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [u'#']),\n",
       " (6, [u'Apache']),\n",
       " (5, [u'Spark']),\n",
       " (5, [u'Spark']),\n",
       " (2, [u'is']),\n",
       " (1, [u'a']),\n",
       " (4, [u'fast']),\n",
       " (3, [u'and']),\n",
       " (7, [u'general']),\n",
       " (7, [u'cluster'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = rdm.flatMap(lambda x: x.split())\n",
    "words = words.map(lambda x: (len(x),x))\n",
    "words.mapValues(lambda x : list([x])).collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = rdm.flatMap(lambda x: x.split())\n",
    "words = words.map(lambda x: (x,1))\n",
    "res = words.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
    "res.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CombineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (68,\n",
       "   u', , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ')),\n",
       " (112,\n",
       "  (1,\n",
       "   u'[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)')),\n",
       " (2,\n",
       "  (70,\n",
       "   u'is, It, in, R,, an, It, of, ##, on, ##, is, To, do, to, do, if, by, -T, in, is, at, an, ##, to, is, to, ##, if, ##, in, To, of, Pi, to, to, be, or, to, on, to, or, to, an, if, is, in, of, if, no, ##, is, be, on, to, or, ##, to, to, in, of, to, at, on, of, ##, to, in, an, on, to')),\n",
       " (4,\n",
       "  (47,\n",
       "   u'fast, APIs, that, data, also, rich, find, This, file, only, run:, (You, need, this, more, than, with, More, from, IDE,, also, also, with, will, when, This, URL,, with, with, also, name, Many, help, Once, [run, Note, uses, core, talk, HDFS, have, must, same, that, your, Hive, Hive')),\n",
       " (6,\n",
       "  (41,\n",
       "   u'Apache, system, Scala,, engine, graphs, GraphX, stream, Online, latest, guide,, README, thread, option, Maven,, builds, shell:, should, return, scala>, Python, prefer, Python, shell:, should, return, sample, MASTER, submit, \"yarn\", params, given., built,, using:, Please, Hadoop, Hadoop, Please, Hadoop, Please, online, Spark.'))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/README.md'\n",
    "lines = sc.textFile(PATH)\n",
    "words = lines.flatMap(lambda x : x. split(' '))\n",
    "words = words.map(lambda x : (len(x),x))\n",
    "\n",
    "comp = words.combineByKey(\n",
    "                    (lambda x : (1,x)),  # sets a default (new nested value)\n",
    "                   (lambda x,y : (x[0] +1 , x[1] +', '+ y)), # appends \n",
    "                   (lambda x,y : (x[0] + y[0], x[1] +', '+y[1]))) # adds all the lists together across partitions\n",
    "comp.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubtractByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "PATH1 = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/supervisor_sf.csv'\n",
    "PATH2 = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/filtered_registered_business_sf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines1 = sc.textFile(PATH1)\n",
    "lines2 = sc.textFile(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94102', u'8'),\n",
       " (u'94102', u'6'),\n",
       " (u'94102', u'3'),\n",
       " (u'94102', u'5'),\n",
       " (u'94103', u'8')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split by word\n",
    "zippairs = lines1.map(lambda x : x.split(','))\n",
    "\n",
    "# then turn into tuple \n",
    "zippairs = zippairs.map(lambda x : (x[0],x[1]))\n",
    "\n",
    "# look at data\n",
    "zippairs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94123', u'Tournahu George L'),\n",
       " (u'94124', u'Stephens Institute Inc'),\n",
       " (u'94105', u'Stephens Institute Inc'),\n",
       " (u'94108', u'Stephens Institute Inc'),\n",
       " (u'94107', u'Stephens Institute Inc')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the 2nd file by commas\n",
    "zipbus = lines2.map(lambda x : x.split(','))\n",
    "\n",
    "# split the 2nd file by commas\n",
    "zipbus = zipbus.map(lambda x : (x[0], x[1]))\n",
    "zipbus.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Precision Communication Serv',\n",
       " u'Schefer Thomas R',\n",
       " u'Tyrrell Thomas',\n",
       " u'Lucid Systems',\n",
       " u'Jacob Abraham']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = zipbus.subtractByKey(zippairs).values().distinct()\n",
    "res.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_num_pairs = sc.parallelize({(2,3), (1,2),(1,3),(2,4),(3,6)})\n",
    "\n",
    "second_num_pairs = sc.parallelize({(1,3),(2,2),(2,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract (both key and value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (3, 6), (1, 2), (2, 4)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtract(second_num_pairs).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract by Key (key only match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtractByKey(second_num_pairs).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)),\n",
       " (1, (3, 3)),\n",
       " (2, (3, 10)),\n",
       " (2, (3, 2)),\n",
       " (2, (4, 10)),\n",
       " (2, (4, 2))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.join(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)),\n",
       " (1, (3, 3)),\n",
       " (2, (3, 10)),\n",
       " (2, (3, 2)),\n",
       " (2, (4, 10)),\n",
       " (2, (4, 2))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.rightOuterJoin(second_num_pairs).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)),\n",
       " (1, (3, 3)),\n",
       " (2, (3, 10)),\n",
       " (2, (3, 2)),\n",
       " (2, (4, 10)),\n",
       " (2, (4, 2)),\n",
       " (3, (6, None))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.leftOuterJoin(second_num_pairs).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cogroup - no cartesianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (5, 3)), (2, (7, 12)), (3, (6, 0))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.cogroup(second_num_pairs).mapValues(lambda x: (sum(x[0]),sum(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94102', u'8'),\n",
       " (u'94102', u'6'),\n",
       " (u'94102', u'3'),\n",
       " (u'94102', u'5'),\n",
       " (u'94103', u'8')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superzips = lines1.map(lambda x: x.split(','))\n",
    "superzips = superzips.map(lambda x : (x[0],x[1]))\n",
    "superzips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94123', u'Tournahu George L'),\n",
       " (u'94124', u'Stephens Institute Inc'),\n",
       " (u'94105', u'Stephens Institute Inc'),\n",
       " (u'94108', u'Stephens Institute Inc'),\n",
       " (u'94107', u'Stephens Institute Inc')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesszips = lines2.map(lambda x: x.split(','))\n",
    "businesszips = businesszips.map(lambda x : (x[0],x[1]))\n",
    "businesszips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf97d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf9250>)),\n",
       " (u'85716',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf9f10>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf9f50>)),\n",
       " (u'28036',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf9f90>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf9fd0>)),\n",
       " (u'97302',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf6050>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf6090>)),\n",
       " (u'55344',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf6150>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf6190>)),\n",
       " (u'94035',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x10fcf61d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x10fcf6210>))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superzips.cogroup(businesszips).take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'94102': 4,\n",
       "             u'94103': 6,\n",
       "             u'94104': 2,\n",
       "             u'94105': 2,\n",
       "             u'94107': 2,\n",
       "             u'94108': 2,\n",
       "             u'94109': 4,\n",
       "             u'94110': 4,\n",
       "             u'94111': 2,\n",
       "             u'94112': 5,\n",
       "             u'94114': 3,\n",
       "             u'94115': 3,\n",
       "             u'94116': 2,\n",
       "             u'94117': 4,\n",
       "             u'94118': 3,\n",
       "             u'94121': 2,\n",
       "             u'94122': 4,\n",
       "             u'94123': 1,\n",
       "             u'94124': 2,\n",
       "             u'94127': 3,\n",
       "             u'94129': 1,\n",
       "             u'94130': 1,\n",
       "             u'94131': 3,\n",
       "             u'94132': 3,\n",
       "             u'94133': 2,\n",
       "             u'94134': 3,\n",
       "             u'94158': 2})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superzips.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Hard Practice Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from user_definition import *\n",
    "# Do not add any additional libraries.\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(app_name)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "lines = sc.textFile('input.csv')\n",
    "lines.collect()\n",
    "\n",
    "nums = lines.flatMap(lambda x : x.split(','))\n",
    "nums = nums.map(lambda x : int(x))\n",
    "\n",
    "evens = nums.filter(lambda x : x % 2 == 0)\n",
    "evens = evens.sortBy(lambda x : x)\n",
    "max_even = evens.max()\n",
    "min_even = evens.min()\n",
    "\n",
    "even_partitions = range(min_even, max_even, (max_even-min_even)/hist_size )\n",
    "\n",
    "odds = nums.filter(lambda x : x % 2 == 1)\n",
    "odds = odds.sortBy(lambda x: x)\n",
    "max_odd = odds.max()\n",
    "min_odd = odds.min()\n",
    "\n",
    "odd_partitions = range(min_odd, max_odd, (max_odd-min_odd)/hist_size)\n",
    "\n",
    "def assign_even_key(x):\n",
    "    for y in even_partitions[::-1]:\n",
    "        if  x >= y:\n",
    "            return (y,1)\n",
    "        \n",
    "def assign_odd_key(x):\n",
    "    for y in odd_partitions[::-1]:\n",
    "        if  x >= y:\n",
    "            return (y,1)\n",
    "\n",
    "histdodds = odds.map(assign_odd_key)\n",
    "histevens = evens.map(assign_even_key)\n",
    "\n",
    "even_res = dict(histevens.reduceByKey(lambda x, y : x + y).collect())\n",
    "odd_res = dict(histdodds.reduceByKey(lambda x, y : x + y).collect())\n",
    "\n",
    "for x in odd_partitions:\n",
    "    if x not in odd_res:\n",
    "        odd_res[x] =0\n",
    "        \n",
    "for x in even_partitions: \n",
    "    if x not in even_res:\n",
    "        even_res[x] =0\n",
    "print (even_partitions, [even_res[x] for x in even_partitions])\n",
    "print (odd_partitions, [odd_res[x] for x in odd_partitions])\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Concepts\n",
    "\n",
    "\n",
    "    - keys()\n",
    "    - values()\n",
    "    - sortByKey()\n",
    "    - sortBy(lambda x : x[1]) <- can pass other params here to sort\n",
    "    - groupByKey() - > gives key, iterable\n",
    "    - mapValues(func) does not change keys (keeps original partitioning)\n",
    "    - flatMapvalues(func) returns an iterator for each k-v pair,\n",
    "    - reduceByKey(func) - combines values with same key like reduce (horizontal) TRANSFORMATION new RDD\n",
    "    - combineBykey(default_value, in-partition func, across partition_func) - \n",
    "    \n",
    "    - groupByKey searches over partitions, reduce by key does not\n",
    "    \n",
    "    - subtrackByKey\n",
    "    - join\n",
    "    - rightOuterJoin\n",
    "    - leftOuterJoin\n",
    "    - cogroup -> can work on more than 1 RDD at once\n",
    "    \n",
    "    - countByKey()\n",
    "    - lookup(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "[x for x in chunks([1,2,3,4,5,6,7,8,9,10],3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSAN 604 Sec2Lec2 Notes: RDDs, Transformations, Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to run the code, please note where your dataset is and also that you have started the notebook at the command line using:\n",
    "\n",
    "```bash\n",
    "$pyspark\n",
    "```\n",
    "\n",
    "which should open jupyter notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.1.45.27:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a RDD?\n",
    "\n",
    "\n",
    "![alt text](http://backtobazics.com/wp-content/themes/twentyfourteen/images/spark/spark-rdd.png)\n",
    "\n",
    "RDD is a distributed file set split over partitions in a data cluster. This is represented in spark as a single dataset called RD.\n",
    "\n",
    "**Partition**: a \"subset\". In computer terms, you may have 1TB, but you can put some walls up to subdivide the drive into 2 drives of 500 GB, or 3 drives of 250, 250, 500 GB.\n",
    "\n",
    "** By default, max number of partitions is number of threads that you computer has **\n",
    "** Example: if your computer has a quad core intel, each has 2 threads so 4 x2 = 8 available cores, 8 partitions**\n",
    "\n",
    "Some features:\n",
    "\n",
    "1. Distributed -> over different partitions\n",
    "2. Immutable -> read only\n",
    "3. Resilient -> if one node dies, cluster will be rebuilt. (Not replication). The data will rebuilt by the instructions supplied by the master note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulations - Transformations + Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/3QiV8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD - Transformations: change RDD's and return RDDs\n",
    "\n",
    "### RDD - Actions: take RDD's and return values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://trongkhoanguyen.com/assets/post-images/2014/rdd-operations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/README.md'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from readme markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/README.md MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(filepath)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[9] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"spark\",\"spark is fun!\"])\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Shows all the partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - **`collect`** is a mapping function that is forcing a evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], ['spark'], [], [], [], ['spark is fun!']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Shows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'spark is fun!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets add more data\n",
    "\n",
    "- add an option for how many partitions it needs to be divided by\n",
    "- sc.parallelize( data, no_of_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['spark', 'spark is fun!'], ['1', '2'], ['3', '4'], ['5', '6', '7', '8']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"spark\",\"spark is fun!\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"],4)\n",
    "lines.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda functions\n",
    "\n",
    "These are one-time functions are executed at run-time. These are also considered anonymus functions. A lot of times you don't have to name the functions.\n",
    "\n",
    "Example of a regular function\n",
    "```python\n",
    "def f(x):\n",
    "    return x+2\n",
    "```\n",
    "\n",
    "Inline equivalent\n",
    "```python\n",
    "lambda x : x+2\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "1. Transformations\n",
    "\n",
    "**RDDs when changed:** Make a new RDD when manipulating an old RDD (this is because all RDDs are immutable). So any changes must mean a new data set has to be stored to a new dataset.\n",
    "    \n",
    "    \n",
    "    ```python\n",
    "    # only want lines that have the word 'spark\" in it\n",
    "    lines_with_spark = lines.filter(lambda lines : 'spark' in lines)\n",
    "    ```\n",
    "    \n",
    "**Lazy evaluation**: the above filtering function has been setup BUT HAS NOT BEEN EXECUTED. This will only be executed when the `lines_with_spark` is called in another function\n",
    "\n",
    "**Sample Transformation functions:** \n",
    "\n",
    "- map\n",
    "- filter\n",
    "- flatMAP\n",
    "- mapPartitions\n",
    "\n",
    "** Example: Map (on Elements)**\n",
    "\n",
    "\n",
    "Typical methods that take a function and apply element wise\n",
    "- map(func)\n",
    "\n",
    "- flatmap(func)\n",
    "\n",
    "- filter(func)\n",
    "\n",
    "\n",
    "Element wise transformation methods, even across partitions. \n",
    "\n",
    "**Example**, lets say you have a dataset [1,2,3,4] split over 2 partitions:\n",
    "\n",
    "    Partition 1 \n",
    "    1, 2\n",
    "\n",
    "    Partition 2\n",
    "    3, 4\n",
    "\n",
    "`map(f(x))` will do the following:\n",
    "\n",
    "    Partition 1 \n",
    "    f(1), f(2)\n",
    "\n",
    "    Partition 2\n",
    "    f(3), f(4)\n",
    "    \n",
    "**Example** let's say you have a dataset:\n",
    "```python\n",
    "'I love tacos'\n",
    "'I love coffee'\n",
    "```\n",
    "**map function under** `.map(lambda x : x.split())`\n",
    "\n",
    "```python\n",
    "[['I','love','taco']\n",
    "  ['I','love','coffee']]\n",
    "```\n",
    "\n",
    "Note the nested list structure\n",
    "\n",
    "**flat map** under `.flatmap(lambda x : x.split())`\n",
    "\n",
    "\n",
    "```python\n",
    "['I','love','taco','I','love','coffee']\n",
    "```\n",
    "\n",
    "Note the flat list structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inclass Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/ignatian_pedagogy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'= Ignatian Values =',\n",
       " u'The University of San Francisco enjoys a distinguished heritage and Jesuit tradition.  At the core of this tradition are transcendent values, including the integration of learning, faith and service; care for the whole person; character and conviction; religious truth and interfaith understanding; and a commitment to building a more just world.  The key values of this Jesuit tradition are as follows:',\n",
       " u'***********************************************************************************',\n",
       " u\"1. Contemplative in Action - St. Ignatius Loyola believed that prayer and reflectivity should so guide our choices and actions that our activity itself becomes a way of entering into union with and praising God.  Being a contemplative in action also means seeing beyond the superficial in life to appreciate the mystery, beauty, and sacredness of all life.  It is a means of seeing God in all things and in everyone.  Contemplation is a critical dimension of the spiritual life and it is reflected in USF's commitment to prayer and spiritual growth.  Analogously, in the academic life, a spirit of reflectivity is a critical aspect of intellectual inquiry.\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads in a CSV flat text file\n",
    "lines = sc.textFile(filepath)\n",
    "\n",
    "# collects all the different terms\n",
    "lines.collect()[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the words using the map function (should have nested lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'=', u'Ignatian', u'Values', u'='],\n",
       " [u'The',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'San',\n",
       "  u'Francisco',\n",
       "  u'enjoys',\n",
       "  u'a',\n",
       "  u'distinguished',\n",
       "  u'heritage',\n",
       "  u'and',\n",
       "  u'Jesuit',\n",
       "  u'tradition.',\n",
       "  u'At',\n",
       "  u'the',\n",
       "  u'core',\n",
       "  u'of',\n",
       "  u'this',\n",
       "  u'tradition',\n",
       "  u'are',\n",
       "  u'transcendent',\n",
       "  u'values,',\n",
       "  u'including',\n",
       "  u'the',\n",
       "  u'integration',\n",
       "  u'of',\n",
       "  u'learning,',\n",
       "  u'faith',\n",
       "  u'and',\n",
       "  u'service;',\n",
       "  u'care',\n",
       "  u'for',\n",
       "  u'the',\n",
       "  u'whole',\n",
       "  u'person;',\n",
       "  u'character',\n",
       "  u'and',\n",
       "  u'conviction;',\n",
       "  u'religious',\n",
       "  u'truth',\n",
       "  u'and',\n",
       "  u'interfaith',\n",
       "  u'understanding;',\n",
       "  u'and',\n",
       "  u'a',\n",
       "  u'commitment',\n",
       "  u'to',\n",
       "  u'building',\n",
       "  u'a',\n",
       "  u'more',\n",
       "  u'just',\n",
       "  u'world.',\n",
       "  u'The',\n",
       "  u'key',\n",
       "  u'values',\n",
       "  u'of',\n",
       "  u'this',\n",
       "  u'Jesuit',\n",
       "  u'tradition',\n",
       "  u'are',\n",
       "  u'as',\n",
       "  u'follows:'],\n",
       " [u'***********************************************************************************']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.map(lambda line: line.split())\n",
    "\n",
    "# each of the lines is essentially a paragraph\n",
    "words.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclass Problem - do the same above exercise, but do it using a flatMap command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'=',\n",
       " u'Ignatian',\n",
       " u'Values',\n",
       " u'=',\n",
       " u'The',\n",
       " u'University',\n",
       " u'of',\n",
       " u'San',\n",
       " u'Francisco',\n",
       " u'enjoys']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda line: line.split())\n",
    "words.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2 : filter by 'USF'\n",
    "\n",
    "#### using a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"USF's\", u'USF', u\"USF's\", u\"USF's\", u'USF', u'USF', u'USF', u'USF']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda line: [x for x in line.split() if 'USF' in x])\n",
    "words.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"USF's\", u'USF', u\"USF's\", u\"USF's\", u'USF', u'USF', u'USF', u'USF']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda line: [x for x in line.split() if 'USF' in x])\n",
    "filtered_words = words.filter(lambda x: 'USF' in x )\n",
    "filtered_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Based Operations\n",
    "\n",
    "\n",
    "**Remember the Example**, lets say you have a dataset [1,2,3,4] split over 2 partitions:\n",
    "\n",
    "    Partition 1 \n",
    "    1, 2\n",
    "\n",
    "    Partition 2\n",
    "    3, 4\n",
    "\n",
    "`map(f(x))` will do the following:\n",
    "\n",
    "    Partition 1 \n",
    "    f(1), f(2)\n",
    "\n",
    "    Partition 2\n",
    "    f(3), f(4)\n",
    "    \n",
    "Creates a connection & function per element, very computationally expensive. **What about doing it at a partition level ( level above)?**\n",
    "\n",
    "This is done by creating a **iterator**. A iterator is a data structure that works like a cursor. It starts at the beginning of a list and only feeds the next element when the process is ready. Simplest python example is as follows:\n",
    "\n",
    "`[0,1,2,3,4,5,6]` vs. `xrange(6)`\n",
    "\n",
    "\n",
    "### Inclass example: parallelize numbers between 1 and 16. Calculate the count and sum in each partition.\n",
    "\n",
    "In 4 partitions:\n",
    "\n",
    "```python\n",
    "\n",
    "[1,2] [3,4] [ 5,6], [7,8]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "(2,3) (2,7)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make a function for map partioins()\n",
    "\n",
    "lets make a sample numbers dataset for 16 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize(range(1,17))\n",
    "numbers.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sum(nums):\n",
    "    \n",
    "    # making our own list to hold two things\n",
    "    # first the count\n",
    "    # the sum\n",
    "    count_sum = [0,0]\n",
    "    \n",
    "    for num in nums:\n",
    "        # will count the quantity of floats\n",
    "        count_sum[0] += 1\n",
    "        # will sum the actual values of the floats\n",
    "        count_sum[1] += num\n",
    "        \n",
    "    # returning a nested list\n",
    "    return [count_sum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the answer is for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [2, 7], [2, 11], [2, 15], [2, 19], [2, 23], [2, 27], [2, 31]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse = numbers.mapPartitions(count_sum)\n",
    "parse.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to get the total sum overall partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 136]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count_sum = parse.reduce(lambda x,y: [x[0]+y[0],x[1]+y[1]])\n",
    "total_count_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Spark program in a Python script (instead of notebooks)\n",
    "\n",
    "Know this for the test. The submissions are going to be in .py\n",
    "\n",
    "## Within your python file\n",
    "\n",
    "#### import your libraries\n",
    "```python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "```\n",
    "\n",
    "#### Set the configuration files. \n",
    "\n",
    "- **Appname** = the name program or 'job' when its sent to the cluster to be run.\n",
    "- **local[\\*]** = the cluster or program that you are connecting to. Would be a URL if connecting to a system\n",
    "\n",
    "```python\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"AppName\")\n",
    "```\n",
    "\n",
    "#### start your pyspark context (program envir)\n",
    "```python\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# when done call\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "## At the command line\n",
    "\n",
    "#### Check current setup\n",
    "\n",
    "```bash\n",
    "$echo $PYSPARK_DRIVER_PYTHON\n",
    "jupyter\n",
    "\n",
    "$echo $PYSPARK_DRIVER_PYTHON_OPTS\n",
    "notebook\n",
    "\n",
    "```\n",
    "\n",
    "#### Unset your environment variables\n",
    "\n",
    "```bash\n",
    "$unset PYSPARK_DRIVER_PYTHON\n",
    "$echo PYSPARK_DRIER_PYTHON\n",
    "\n",
    "$unset PYSPARK_DRIVER_PYTHON_OPTS\n",
    "$echo PYSPARK_DRIVER_PYTHON_OPTS\n",
    "```\n",
    "\n",
    "#### Run your standalone program\n",
    "```bash\n",
    "# too much output! but will print to screen\n",
    "$spark-submit ex6.py\n",
    "\n",
    "# will still print to console, but the ouput only will write to a file\n",
    "$spark-submit ex6.py > output.txt\n",
    "```\n",
    "\n",
    "#### Reset the environment variables\n",
    "\n",
    "```bash\n",
    "$ nano ~/.bash_profile\n",
    "```\n",
    "\n",
    "#### Within your bash profile - reset the defaults back to jupyter notebook\n",
    "\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON=\"notebook\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of HW1 assignment, and submission\n",
    "\n",
    "Split by word, but if number, add the values together. Going over the assignment PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operation-Transformations\n",
    "\n",
    "- distinct()\n",
    "- union()\n",
    "- intersection()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operation-Action\n",
    "\n",
    "Compute on RDD, but return non-RDD answers. \n",
    "\n",
    "- only those covered in class will be on quiz (see below)\n",
    "```python\n",
    "reduce()\n",
    "collect()\n",
    "count()\n",
    "```\n",
    "\n",
    "others\n",
    "```\n",
    "fold()\n",
    "aggregate()\n",
    "```\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    ">>> num = [1, 2] [3, 4] [5, 6] [7, 8]\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> num.reduce(lambda x,y: x + y)\n",
    "```\n",
    "\n",
    "What happens under the hood\n",
    "\n",
    "`[1 + 2] [3 + 4] [5 + 6] [7 + 8]`\n",
    "\n",
    "`[x + y] [x + y] [x + y] [x + y]`\n",
    "\n",
    "Reduces x's and y's by partition\n",
    "\n",
    "`[3] [7] [11] [15]`\n",
    "\n",
    "`[3]+[7] <-[11]<- [15]`\n",
    "\n",
    "\n",
    "\n",
    "Then condenses (there's a lot of options here, so not guaranteed to be the exact process)\n",
    "\n",
    "    `[10]<-[11]`\n",
    "\n",
    "        `[21]<- [15]`\n",
    "\n",
    "            `[36]`\n",
    "\n",
    "** Fold example **\n",
    "\n",
    "```python\n",
    ">>> num.fold(0)(lambda x,y: x + y)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "What happens under the hood, note the empty partition on the right. The fold(0) will give a default value and prevent a computation error\n",
    "\n",
    "`[1 + 2] [3 + 4] [5 + 6] [7 + 8] []`\n",
    "\n",
    "`[0+ x + y] [0 + x + y] [0 + x + y] [0 + x + y] [0]`\n",
    "\n",
    "Reduces x's and y's by partition\n",
    "\n",
    "`[3] [7] [11] [15] [0]`\n",
    "\n",
    "`[3]+[7] <-[11]<- [15] <- [0]`\n",
    "\n",
    "\n",
    "\n",
    "Then condenses\n",
    "\n",
    "    `[10]<-[11]`\n",
    "\n",
    "        `[21]<- [15]`\n",
    "\n",
    "            `[36]`\n",
    "\n",
    "\n",
    "** In class Example 4-1 calculate sum of odd nubmers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize(range(1,17))\n",
    "numbers.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOdd(x):\n",
    "    if x % 2 == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = numbers.map(lambda x: x if x %2==1 else 0) \n",
    "temp1 = numbers.filter(lambda x: x%2==1) \n",
    "temp2 = temp1.reduce(lambda x,y : x+y)\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda x,y : 0 if x&2 == 0 else 0 if y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD operation-actions (to be used with reduce)\n",
    "\n",
    "Numeric RDD action Types\n",
    "\n",
    "- count()\n",
    "- collect()\n",
    "- countByValue()\n",
    "- top(n)\n",
    "- take(n)\n",
    "- first()\n",
    "- takeSample()\n",
    "- foreach()\n",
    "- mean()\n",
    "- sum()\n",
    "- max()\n",
    "- min()\n",
    "- variance()\n",
    "- stdev()\n",
    "\n",
    "Sample for a RDD\n",
    "\n",
    "```python\n",
    "rdd_variable.sum()\n",
    "rdd_variable.top()\n",
    "rdd_variable.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inclass exercise - the number of distinct values \n",
    "\n",
    "from a pedagogy file in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/tlee010/Desktop/github_repos/2017-msan694-example/Data/ignatian_pedagogy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pedagog = sc.textFile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'all', 3),\n",
       " (u'enrollment.', 1),\n",
       " (u'themes', 1),\n",
       " (u'religious', 2),\n",
       " (u'Today', 1),\n",
       " (u'relationships', 1),\n",
       " (u'young', 1),\n",
       " (u'to', 17),\n",
       " (u'Reflecting', 1),\n",
       " (u'discovering', 1)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pedagog.flatMap(lambda x : x.split())\n",
    "summary = words.countByValue()\n",
    "summary.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
